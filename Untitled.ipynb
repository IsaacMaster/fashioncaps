{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import argparse\n",
    "import torch\n",
    "import torch.utils.data.dataloader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "import time\n",
    "import torch.legacy.nn as legacy_nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class pseudoInverse(object):\n",
    "    def __init__(self,params,C=1e-2,forgettingfactor=1):\n",
    "        self.params=list(params)\n",
    "        self.is_cuda=self.params[len(self.params)-1].is_cuda\n",
    "        self.C=C\n",
    "        #self.params[len(self.params)-1].data.fill_(0)\n",
    "        self.w=self.params[len(self.params)-1]\n",
    "        self.w.data.fill_(0)#initialize output weight as zeros\n",
    "        # For sequential learning in OS-ELM\n",
    "        self.dimInput=self.params[len(self.params)-1].data.size()[1]\n",
    "        self.forgettingfactor=forgettingfactor\n",
    "        self.M=Variable(torch.inverse(self.C*torch.eye(self.dimInput)))\n",
    "\n",
    "        if self.is_cuda:\n",
    "            self.M=self.M.cuda()\n",
    "\n",
    "    def initialize(self):\n",
    "        self.M = Variable(torch.inverse(self.C * torch.eye(self.dimInput)))\n",
    "\n",
    "        if self.is_cuda:\n",
    "            self.M = self.M.cuda()\n",
    "        self.w = self.params[len(self.params) - 1]\n",
    "        self.w.data.fill_(0.0)\n",
    "\n",
    "\n",
    "    def train(self,inputs,targets):\n",
    "        oneHotTarget=self.oneHotVectorize(targets=targets)\n",
    "        numSamples=inputs.size()[0]\n",
    "        dimInput=inputs.size()[1]\n",
    "        dimTarget=oneHotTarget.size()[1]\n",
    "\n",
    "\n",
    "        xtx= torch.mm(inputs.t(),inputs)\n",
    "\n",
    "        I = Variable(torch.eye(dimInput))\n",
    "        if self.is_cuda:\n",
    "            I=I.cuda()\n",
    "\n",
    "        self.M = Variable(torch.inverse(xtx.data+self.C*I.data))\n",
    "        w = torch.mm(self.M,inputs.t())\n",
    "        w = torch.mm(w,oneHotTarget)\n",
    "\n",
    "        #self.params[len(self.params)-1].data=w.t().data\n",
    "        self.w.data=w.t().data\n",
    "\n",
    "    def train_sequential(self,inputs,targets):\n",
    "        oneHotTarget = self.oneHotVectorize(targets=targets)\n",
    "        numSamples = inputs.size()[0]\n",
    "        dimInput = inputs.size()[1]\n",
    "        dimTarget = oneHotTarget.size()[1]\n",
    "\n",
    "\n",
    "        I = Variable(torch.eye(numSamples))\n",
    "        if self.is_cuda:\n",
    "            I = I.cuda()\n",
    "\n",
    "        self.M = (1/self.forgettingfactor) * self.M - torch.mm((1/self.forgettingfactor) * self.M,\n",
    "                                             torch.mm(inputs.t(), torch.mm(Variable(torch.inverse(I.data + torch.mm(inputs, torch.mm((1/self.forgettingfactor)* self.M, inputs.t())).data)),\n",
    "                                             torch.mm(inputs, (1/self.forgettingfactor)* self.M))))\n",
    "\n",
    "\n",
    "        self.w.data += torch.mm(self.M,torch.mm(inputs.t(),oneHotTarget - torch.mm(inputs,self.w.t()))).t().data\n",
    "\n",
    "\n",
    "    def oneHotVectorize(self,targets):\n",
    "        oneHotTarget=torch.zeros(targets.size()[0],targets.max().data[0]+1)\n",
    "\n",
    "        for i in range(targets.size()[0]):\n",
    "            oneHotTarget[i][targets[i].data[0]]=1\n",
    "\n",
    "        if self.is_cuda:\n",
    "            oneHotTarget=oneHotTarget.cuda()\n",
    "        oneHotTarget=Variable(oneHotTarget)\n",
    "\n",
    "        return oneHotTarget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_StoreAction(option_strings=['--seed'], dest='seed', nargs=None, const=None, default=2, type=<class 'int'>, choices=None, help='random seed (default: 1)', metavar='S')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training settings\n",
    "parser = argparse.ArgumentParser(description='PyTorch ELM MNIST Example')\n",
    "parser.add_argument('--batch-size', type=int, default=60000, metavar='N',\n",
    "                    help='input batch size for training (default: 64)')\n",
    "parser.add_argument('--test-batch-size', type=int, default=100, metavar='N',\n",
    "                    help='input batch size for testing (default: 1000)')\n",
    "#parser.add_argument('--epochs', type=int, default=10, metavar='N',\n",
    "#                   help='number of epochs to train (default: 10)')\n",
    "#parser.add_argument('--lr', type=float, default=0.01, metavar='LR',\n",
    "#                   help='learning rate (default: 0.01)')\n",
    "#parser.add_argument('--momentum', type=float, default=0.5, metavar='M',\n",
    "#                   help='SGD momentum (default: 0.5)')\n",
    "parser.add_argument('--no-cuda', action='store_true', default=False,\n",
    "                    help='enables CUDA training')\n",
    "parser.add_argument('--seed', type=int, default=2, metavar='S',\n",
    "                    help='random seed (default: 1)')\n",
    "#parser.add_argument('--log-interval', type=int, default=10, metavar='N',\n",
    "#                    help='how many batches to wait before logging training status')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training time: 164.35sec\n",
      "\n",
      "Train set accuracy: 56300/60000 (94%)\n",
      "\n",
      "\n",
      "Test set accuracy: 8872/10000 (89%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1345)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.FashionMNIST('../data', train=True, download=True,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0,), (1,))\n",
    "                   ])),\n",
    "    batch_size=60000, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.FashionMNIST('../data', train=False, transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0,), (1,))\n",
    "                   ])),\n",
    "    batch_size=60000, shuffle=True)\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(28*28, 9000)\n",
    "        #self.bn = nn.BatchNorm1d(7000)\n",
    "        self.fc2 = nn.Linear(9000, 10, bias=False) # ELM do not use bias in the output layer.\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "\n",
    "        x = self.fc1(x)\n",
    "        #x = self.bn(x)\n",
    "        x = F.leaky_relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "    def forwardToHidden(self, x):\n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        x = self.fc1(x)\n",
    "        #x = self.bn(x)\n",
    "        x = F.leaky_relu(x)\n",
    "        return x\n",
    "\n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features\n",
    "\n",
    "model = Net()\n",
    "\n",
    "optimizer= pseudoInverse(params=model.parameters(),C=7e-3)\n",
    "\n",
    "\n",
    "def train():\n",
    "    init = time.time()\n",
    "    model.train()\n",
    "    correct = 0\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = Variable(data), Variable(target)\n",
    "\n",
    "        hiddenOut = model.forwardToHidden(data)\n",
    "        optimizer.train(inputs=hiddenOut, targets=target)\n",
    "        output = model.forward(data)\n",
    "        pred=output.data.max(1)[1]\n",
    "        correct += pred.eq(target.data).cpu().sum()\n",
    "    ending = time.time()\n",
    "    print('training time: {:.2f}sec'.format(ending - init))\n",
    "    print('\\nTrain set accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        correct, len(train_loader.dataset),\n",
    "        100. * correct / len(train_loader.dataset)))\n",
    "\n",
    "def train_someBatch(batchidx=0):\n",
    "    init = time.time()\n",
    "    model.train()\n",
    "    correct = 0\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = Variable(data), Variable(target)\n",
    "\n",
    "        if batch_idx == batchidx:\n",
    "            hiddenOut = model.forwardToHidden(data)\n",
    "            optimizer.train(inputs=hiddenOut, targets=target)\n",
    "        output = model.forward(data)\n",
    "        pred=output.data.max(1)[1]\n",
    "        correct += pred.eq(target.data).cpu().sum()\n",
    "\n",
    "    ending = time.time()\n",
    "    print('training time: {:.2f}sec'.format(ending - init))\n",
    "    print('\\nTrain set accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        correct, len(train_loader.dataset),\n",
    "        100. * correct / len(train_loader.dataset)))\n",
    "\n",
    "\n",
    "def test():\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    for data, target in test_loader:\n",
    "        data, target = Variable(data, volatile=True), Variable(target)\n",
    "        output = model.forward(data)\n",
    "        pred=output.data.max(1)[1]\n",
    "        correct += pred.eq(target.data).cpu().sum()\n",
    "    print('\\nTest set accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "\n",
    "def train_sequential(starting_batch_idex=0):\n",
    "    model.train()\n",
    "    correct = 0\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        if batch_idx >= starting_batch_idex:\n",
    "            data, target = Variable(data, volatile=True), Variable(target)\n",
    "            hiddenOut = model.forwardToHidden(data)\n",
    "            optimizer.train_sequential(hiddenOut,target)\n",
    "\n",
    "\n",
    "            output=model.forward(data)\n",
    "            pred=output.data.max(1)[1]\n",
    "            correct += pred.eq(target.data).cpu().sum()\n",
    "            print('\\n{}st Batch train set accuracy: {}/{} ({:.0f}%)\\n'.format(batch_idx,\n",
    "                correct, (train_loader.batch_size*(batch_idx+1)),\n",
    "                100. * correct / (train_loader.batch_size*(batch_idx+1))))\n",
    "\n",
    "            test()\n",
    "\n",
    "\n",
    "# Basic ELM. Note that this is non-iterative learning;\n",
    "# therefore batch-size is the same as # of training samples.\n",
    "train()\n",
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('OSELM:')\n",
    "# Online Sequential ELM, batch_size is resized.\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.FashionMNIST('../data', train=True, download=True,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0,), (1,))\n",
    "                   ])),\n",
    "    batch_size=1000, shuffle=True)\n",
    "\n",
    "train_someBatch(batchidx=0) #initialize phase; offline batch training\n",
    "train_sequential(starting_batch_idex=1) # Sequential learning phase; online sequential training\n",
    "test()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
